Source: Video: Can ChatGPT o1 actually *think*?
URL: https://www.youtube.com/watch?v=fwo5OST3Jh8

 OpenAI just released their new O1 series of models and all of their marketing around this has been saying that these models can actually reason and think which that's a huge claim to make. If you've been following large language models at all, you know that they're spicy auto complete. They're next word predictors. They don't have like an understanding of the world. They have a statistical model that tells them how different words relate to one another, but they aren't actually using reasoning and logic. So for OpenAI to say that these new models can use reasoning and logic is like a huge huge claim. And it would mean that they're not using the same architecture that all the existing large language models are using. So is that true? Are they doing something totally novel and unique? And are the O1 models really thinking? If you're new here at Alumburta, I'm a software engineer. I work on applications of AI, which I feel like every software engineer at this point works on AI and some capacity or they got laid off. I make a lot of critical tech content so if you like that, please follow along. And that's spoilt video you guys, but a computer can't think. Okay, sorry. You see the word think has a real definition. It's like a real word, you know? It means like stuff going on up here, but in OpenAI world, the word think has a new meaning. The word think means the computer does some mummo jumbo before it responds to you with text. And in my mind, that's not thinking. You guys just made up a new definition of the word, which is not allowed. To me, them saying their model can think is the same as someone telling me their toaster oven can think. It's like, obviously we're just using the word in two different ways because I know that no matter how smart your toaster oven is, it's not thinking. Okay, maybe it's calculating. It's calculating how much more time the bread needs to be in the oven to become toast, but it's not thinking. And if you're calling that thinking, we're just using the word in two different ways. Which is okay. I mean, it's not so okay. It's like a little insidious to be telling people that AI can think now. I will say though the use of the word thinking is not necessarily OpenAI is fault. OpenAI did not invent these like human metaphors. They did popularize them a lot, but we have been talking about AI in human terms, like basically since the inception of AI, we're like constantly using human language. We're constantly anthropomorphizing these models. We're constantly saying, oh, the model is learning. The model is not learning. The model is like taking in new data and like updating weights, even the word neural net. It's like a little misleading, right? Because there's no white matter inside of the computer, right? My head is not being transported into the motherboard. A neural net is a statistical model. It's math. It's numbers. It's weights. It's not an actual brain. And it's not even a replica, an exact replica of a brain. A lot of times people will say, oh, like what we learn about neural nets, we learn about human brains. That's not freaking true. Sometimes there happens to be overlap for some specific things, but not in general. You cannot say everything you know about a neural net applies to the brain. Like those are simply two different things. They were designed by two different people. One was designed by, you know, depending on your religion, Jesus, age, Christ, and one was designed by this, some PhD dude. Okay, so with what said, has OpenAI found a way to distinguish itself from all other AI models that can't think from every other toaster oven out there? Not really. No. They are saying that this architecture is like super different from what they currently have. It's pretty similar. So what is primarily different about this architecture compared to the architecture of GPT-4 and 40 and like all the other large language models? Primarily the difference is these things called reasoning tokens. What are reasoning tokens? I don't know. They won't tell us. They are not telling us. I read the 43 page PDF about the model and it does not explain what a reasoning token is. However, I'm going to take a reasonable guess. A reasoning token I can assume is taking the prompt, breaking it down, having that text beat then tokenize. Same way that they tokenize the input prompt text and then taking all that and doing next word prediction. Basically the idea here is that instead of only generating text to your response, they are first generating something else called a reasoning token, which that's the thinking, right? Either way, when you go to the model, like the text that you're seeing here, it's not actually the reasoning tokens. It's a summarization of the reasoning. So the reasoning itself might be like a little bit more wild. You can imagine you say like how many ours are in the word strawberry? Well, like step by step be like first, break down the letters in the word strawberry and then count the letters in the word strawberry, which is like pretty close to what it actually does. This is actually really, really close to a prompting technique that already exists called chain of thought prompting. And they call that out. Like they're not hiding that they're using this prompting technique that already exists. Basically they're baking in that prompting technique into the model. If you've never used chain of thought prompting, it's asking the model to break down your question into step by step basis. And seeing if it can get it right by like following this reasoning a little bit better. And the reason this works is because when the model has to explain what it's doing at each step, each step has to sort of make sense as a coherent sentence. So it's a little bit harder for it to do something that is totally unreasonable when every single sentence along the way has to make sense. Even though it is using an existing prompting technique and using that baking it into the model, it is still very impressive. You know, they're saying that they got these really high scores on all these benchmark tests. You know, I think they said that they like 49% dialed the math lumpy add. Although I think they did say they were like 10,000 submissions, which I'm like, if I had 10,000 submissions, I could probably get it right too, because that's just guessing at that point. It isn't a very impressive model, but this is not reasoning. This is not thinking. The thinking here is just more text, more tokens. To me, reasoning is like using the laws of logic. Like all dogs are animals. If this thing is a dog, then it's an animal. It's not something an element is ever going to be able to do with 100% certainty. It's not getting us to a point where we are going to have a guarantee that this thing is going to be right. We're not solving hallucinations. We're not following the laws of logic. It is predicting the next word. It's not fully understanding what the frick a dog in an animal is. So given that it's not reasoning, how do we get to this point where this model got so hyped? And this is kind of the most fun part of the video, guys. The podium of this model was strawberry, which was named after how many ours are in the word strawberry is stumping all of the major AI models right now. I didn't come up with that problem. I wish I did. This was a notorious problem that AI models were not able to solve, which by the way, this model sometimes still can't solve it. But the hype was all around that this model was going to be the one that was able to solve this problem with all of its reasoning and thinking and big brainedness. And people on Twitter would not stop posting about strawberries. Sam Allman posted about strawberries. People that worked at OpenAI were posting about strawberries. And in particular, this one man whose name on Twitter was just three strawberry emojis would not stop posting about strawberries. In this one to eat, he posted attention is not all you need, which is a reference to the paper attention is all you need, which basically was the landmark paper that brought us to this like revolutionary place in AI that we're currently at. And in one of the best Reddit comments have ever seen this guy said attention is all this guy needs. It got to a point where Mr. Strawberry man started making predictions about the strawberry model. And people started to think that he actually worked at OpenAI. This guy, even though his tweets are like kind of off the walls and I don't know why anyone really assume he works at OpenAI. Because if he did, I feel like he'd be fired. Started like leaking all this information about the model. And then he started making specific predictions like he was a Doomsday truther and he said August 13th. That is when the model's coming out 10 am Pacific Standard Time. I don't know why he said that date, but then that date came in past. And there was no model. Which I don't like, why did he do that? He really like dug himself a hole there. I don't know, maybe he was just like tired of pretending to be some legitimate. But that did not stop people from posting more pictures of strawberries and you know, posting more hype. By the time this model released, it was heavily anticipated and people were thinking it was going to be totally different. Could do this reasoning. So when they said in the announcement that it can be reasoning, people were losing their minds. And hey, it kind of reasoning, sort of, kind of, in the way that a toaster can. Which is not, not nothing. And it definitely gets us closer to a place where the models are more intelligent. Where they're right more often. Where they're solving more difficult problems. And all that is good. It's just a little bit dangerous. If you don't know the details here and you think that this model can actually think. And therefore, when it gives you an answer to a hard problem, you think it's going to be guaranteed to be correct. Guys, I know it's an impressive model. It is very cool and amazing how much progress AI has made. Open AI is an amazing company. They're doing revolutionary breakthrough work. It's a good model. It's a good model. Okay, stop coming at me in the comments and saying that I don't understand how amazing these breakthroughs are. I get it, I get it. But when we have marketing and communications to non-technical people saying that these kind of models are thinking and reasoning and logic. It's a hype cycle. It's creating hype that is not reflective of what the actual product is capable of. In layman's terms, no, the model cannot think. It really only makes sense to be saying that Cheshire PT can think. When you're talking to other technical people who already know computers can't think. Anyway, that's all I have for you. But you guys should definitely subscribe and click any good button in the general vicinity of the video. By the way, if you're wondering what the O stands for, it stands for, oh, that's the model you guys keep calling the Shawberry model on Twitter. Or at least that's my best guess. And Open AI has a lot of really smart people in the engineering department. But not as many smart people in the naming department. It seems okay, you can leave. You can go now.